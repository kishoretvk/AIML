{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice_Exercise_NLP_week_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys_9ziILz2V_",
        "colab_type": "text"
      },
      "source": [
        "### We cannot work with the text data in machine learning so we need to convert them into numerical vectors, As a part of this practice exercise you will implement different techniques to do the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO_U1p5H0A70",
        "colab_type": "text"
      },
      "source": [
        "### In this notebook we are going to understand techniques for encoding text data. We are going to learn about\n",
        "\n",
        "1. **Techniques for Encoding** - These are the popular techniques that are used for encoding:\n",
        "    *           **Bag of Words**\n",
        "    *           **TF-IDF**( **T**erm  **F**requency - **I**nverse **D**ocument **F**requency)\n",
        "2. **Sentiment Analysis** - Sentiment Analysis is the process of ‘computationally’ determining whether a piece of writing is positive, negative or neutral. The below can be used for Sentiment Analysis:\n",
        "    *           **TextBlob**         \n",
        "    *           **VADER Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q0rVye0zGNA",
        "colab_type": "code",
        "outputId": "408cc4bb-8d7b-4cdc-fbce-37f5577db043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import re\n",
        "import numpy as np                                  #for large and multi-dimensional arrays\n",
        "import pandas as pd                                 #for data manipulation and analysis\n",
        "import nltk                                         #Natural language processing tool-kit\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords                   #Stopwords corpus\n",
        "from nltk.stem import PorterStemmer                 # Stemmer\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
        "\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUey4gmv2hOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = 'I enjoy this program.'\n",
        "d2 = 'This program is great.'\n",
        "d3 = 'This product is not great.'\n",
        "d4 = 'I really love this brand.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6_OCome6xgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to set the stopwords' list removing \"not\" from the list of stopwords.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHRkkH3J5mbU",
        "colab_type": "text"
      },
      "source": [
        "### Basic Pre-processing Steps:\n",
        "\n",
        "- Conversion to lowercase.\n",
        "- Removal of punctuation.\n",
        "- Tokenization.\n",
        "- Stopwords removal except the word 'not'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2G8yS9t5Gy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the function to perform the above four steps.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeXcxizn-0-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to preprocess all the four text strings and save the result into new variables. Join all the four resulting lists.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZksdJQSGxp",
        "colab_type": "text"
      },
      "source": [
        "### **BAG OF WORDS**\n",
        "      \n",
        "In BoW we construct a dictionary that contains set of all unique words from our text review dataset. The frequency of the word is counted here. If there are **d** unique words in our dictionary then for every sentence or review the vector will be of length **d** and count of word from review is stored at its particular location in vector. The vector will be highly sparse in such case.\n",
        "      \n",
        "      \n",
        "#### Using scikit-learn's CountVectorizer we can get the BoW and check out all the parameters it consists of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0VtzfH8_KEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to perform Bag of words representation using CountVectorizer().\n",
        "# Print the following: vocabulary, shape of the matrix, type of the vector, the vector as array.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIpeouzfDbZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to set the unique words as vocabulary. Print the vocabulary.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NHba_ClS3qq",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF**\n",
        "\n",
        "**Term Frequency -  Inverse Document Frequency** it makes sure that less importance is given to most frequent words and also considers less frequent words.\n",
        "\n",
        "**Term Frequency** is number of times a **particular word(W)** occurs in a review divided by totall number of words **(Wr)** in review. The term frequency value ranges from 0 to 1.\n",
        "\n",
        "**Inverse Document Frequency** is calculated as **log(Total Number of Docs(N) / Number of Docs which contains particular word(n))**. Here Docs referred as Reviews.\n",
        "\n",
        "\n",
        "**TF-IDF** is **TF * IDF** that is **(W/Wr)*LOG(N/n)**\n",
        "\n",
        "\n",
        " Using scikit-learn's tfidfVectorizer we can get the TF-IDF.\n",
        "\n",
        "So even here we get a TF-IDF value for every word and in some cases it may consider different meaning reviews as similar after stopwords removal. so to over come we can use BI-Gram or NGram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQcrkj2SLWju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to perform Tfidf Vectorization. Print: vocabulary, the value of idfs, shape of the result, the resulting matrix.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28RiwtpkTEco",
        "colab_type": "text"
      },
      "source": [
        "**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqeDC3C2MDry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to print polarity scores using vaderSentiment for all four texts (d1, d2, d3 and d4) separately.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9U88UMLTUyY",
        "colab_type": "text"
      },
      "source": [
        "#### With the help of TextBlob.sentiment() method, we can get the sentiments of the sentences by using TextBlob.sentiment() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFkcWCqZOfEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to print sentiment scores using TextBlob for all four text strings (d1, d2, d3 and d4) separately.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX2oVwcgRk5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}